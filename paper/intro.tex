Optimization problems come in various flavors that require specific solutions.
In this paper, we study Bayesian optimization applied to black-box sample-efficient optimization problems.
That is, problems for which the only way we can access the underlying behavior is through expensive trials.
For example, we might be trying to find the best location for wildlife sensors or (as we will see later) the proportion of ingredients to make the best mug cakes.

More formally, we are considering a space $\mathcal{X}$ (most often a compact subset of $\mathbb{R}^{K}$) and an unknown continuous function $f: \mathcal{X} \to \mathbb{R}$.
The only structure we require of $f$ is that it be continuous so that we can learn about $f$'s global behavior by evaluating it at a finite number of points.
Our goal is to estimate
\begin{equation}\label{eq:obj}
    \argmax_{\mathbf{x} \in \mathcal{X}} f(\mathbf{x})
\end{equation}
in as few samples as possible 
(Note that the maximum exists because $f$ is continuous and $\mathcal{X}$ is compact).
In the context of baking, $\mathcal{X}$ is a set of recipes and $f$ gives the quality of a recipe after baking.
Further, baking satisfies the continuity requirement because small changes in a recipe have small changes in the quality of the resulting cake.

We will first review Gaussian distributions.
Next, we will explore Gaussian Processes and how to deal with noisy and biased observations
so that we can use them as a surrogate model.
In section ... we will look at acquisition functions.
Having explored both components of Bayesian Optimization we will see how the algorithm performs in the real-world example of finding the perfect mug cake.
Finally, we will investigate how to perform inference on the biases using data from the experiment.
