\subsection{Gaussian Processes}

Gaussian Processes are an extension of Gaussian distributions to random continuous functions $f: \mathcal{X} \to \mathbb{R}$.
That is, for each element $\mathbf{x} \in \mathcal{X}$, we have a random variable $f(\mathbf{x})$.
Working with such a distribution might seem overwhelming, especially if $\mathcal{X}$ has uncountable infinitely many elements.
As such, it might seem like we have to create a joint distribution over a possibly uncountably infinite collection of random variables.
The key to making this problem tractable is that we only consider $f$ at a finite number of points and showing that a corresponding joint exists.

\begin{definition}[Gaussian Process]\label{def:gp}
    A Gaussian Process on $\mathcal{X}$ is a random function $f$ parameterized by a mean function
    $m: \mathcal{X} \to \mathbb{R}$ and covariance/kernel function $\kappa: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$
    such that
    \begin{align*}
        \mathbb{E}[f(\mathbf{x})] &= m(\mathbf{x}) \\
        \mathbb{E}[(f(\mathbf{x}) - m(\mathbf{x}))(f(\mathbf{x}') - m(\mathbf{x}'))] &= \kappa(\mathbf{x}, \mathbf{x}').
    \end{align*}
    Then, for any $\mathbf{x}_1, \ldots,\mathbf{x}_N \in \mathcal{X}$,
    \begin{equation*}
        \mathbf{f} \sim \mathcal{N}_N\left(\mathbf{m}, \mathbf{K}\right)
    \end{equation*}
    where
    \begin{equation*}
        \mathbf{f} = (f(\mathbf{x}_1), \ldots, f(\mathbf{x}_n)), \mathbf{m} = (m(\mathbf{x}_1), \ldots, m(\mathbf{x}_n)), (\mathbf{K})_{ij} = \kappa(\mathbf{x}_i, \mathbf{x}_j)
    \end{equation*}
    
    For this equation to make sense, we require $\kappa$ to generate positive semi-definite $\mathbf{K}$.
    We denote this as $f \sim GP_{\mathcal{X}}(m, \kappa)$.
\end{definition}

We see that there is a one-to-one correspondence between the parameters of a Multivariate Gaussian $\mathcal{N}_{K}(\bsy{\mu}, \bsy{\Sigma})$
and that of a Gaussian process $GP_{ \mathcal{X}}(m, \kappa)$.
The set $\mathcal{X}$ correspondes to $K$ as they both represent the amount of random variables.
The function $m$ corresponds with $\bsy{\mu}$ in that they both give us the means of our random variables.
The kernel $\kappa$ corresponds with $\bsy{\Sigma}$ as they both tell us the covariance between random variables.
They two distribution are also similar in that they have the property that marginals of a finite number of random variables is a Gaussian Distribution.
The key difference is that with Gaussians Distributions, this property of finite marginals is a \emph{result} derived from the full joint distribution
while Gaussian Processes are \emph{defined} by this property.

In Definition~\ref{def:gp}, we asserted that finite marginals are Gaussian, but we still need to verify that these marginals are consistent with each other.
For example, given random variables $X, Y, Z$, it make no sense to say that the marginals are
\begin{align*}
    (X, Y) = (0, 0) \text{ with probability 1} \\
    (Y, Z) = (1, 1) \text{ with probability 1}
\end{align*}
because then $Y$ must equal both 0 and 1.
Similarly, given a finite collection $(f(\mathbf{x}_1), \ldots, f(\mathbf{x}_n))$, Definition~\ref{def:gp} merely defines a function
\begin{equation*}
    \mu_{\mathbf{x}_1, \ldots, \mathbf{x}_n}(E) = \int \ldots
\end{equation*}
that measures how likely $(f(\mathbf{x}_1), \ldots, f(\mathbf{x}_n))$ is in a Borel set $E$.
(The requirement that $E$ be Borel is crucial theoretically, but has little practical implication since
``any subset ... that you can write down in a concrete fashion is a Borel set'' \cite{axler2020}).
For the marginals from Definition~\ref{def:gp} to be consistent, 
we must prove the existence of a probability space that generates such marginals.
The Kolmogorov Extension Theorem helps do just this

\begin{theorem}[Kolmogorov Extension Theorem]\label{thm:kol-ext}
    
\end{theorem}

\begin{theorem}\label{thm:gp-const}
    Gaussian Processes are consistent
\end{theorem}
\begin{proof}
    TODO
\end{proof}

With a solid foundation on Gaussian Processes, we can interpret the parameters of a Gaussian Process.
The intuition behind Gaussian Processes is that if $\mathbf{x}$ and $\mathbf{x}'$ are similar, then
$f(\mathbf{x})$ and $f(\mathbf{x}')$ should be similar.
The parameter $\kappa$ describes this similarity by being high when $\mathbf{x}$ and $\mathbf{x}'$ are similar.
The more similar $\mathbf{x}$ and $\mathbf{x}'$ are, the higher the value of $\kappa(\mathbf{x}, \mathbf{x}')$.
After all, we expect the covariance between $f(\mathbf{x})$ and $f(\mathbf{x}')$
to be high if $\mathbf{x}$ and $\mathbf{x}'$ are similar.

The parameter $m$ tells us our prior expectation of $f$.
Although $m$ can be useful if we have prior knowledge of $f$, it is common to set $m(\mathbf{x}) = 0$ \cite{murphy2012}.
In Figure \ref{fig:gp-sample}, we use a kernel such $\mathbf{x}$-values yield correlated $f(\mathbf{x})$-values.
As such, we see that points close to each other have similar values (e.g. continuity).
We will later show that this is no coincidence.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{fig/gp-sample2d.png}
         \caption{}
         \label{subfig:2d-gp-sample}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{fig/gp-sample3d.png}
         \caption{}
         \label{subfig:3d-gp-sample}
     \end{subfigure}
     \hfill
    \caption{Samples of a Gaussian Process using a RBF kernel with parameters $\ell^{2} = 0.1^2$ and $\sigma^{2}_f = 1 ^ 2$ with
        $\mathcal{X} = [0, 1]$ (a) and $\mathcal{X} = [0, 1]^2$ (b).
    }
    \label{fig:gp-sample}
\end{figure}

\subsubsection{Posterior Inference on GP}

We now turn our attention to posterior inference.
That is, we want to quantify our belief of $f$ given some observations.
Suppose we observe that 
\begin{equation*}
    \mathbf{f} = (f(\mathbf{x}_1), \ldots f(\mathbf{x}_N)) = \mathbf{y} = (y_1, \ldots, y_N).
\end{equation*}

Then, for some points of interest $\mathbf{v}_{1}, \ldots, \mathbf{v}_{N_{*}}$, the joint distribution of $\mathbf{f}$ and $\mathbf{f}_* = (f(\mathbf{v}_1), \ldots, f(\mathbf{v}_{N_*}))$ is
\begin{equation*}
    \begin{bmatrix}
        \mathbf{f} \\
        \mathbf{f}_{*} \\
    \end{bmatrix} \sim
    \mathcal{N}\left(
    \begin{bmatrix}
        \mathbf{m} \\ \mathbf{m}_{*}
    \end{bmatrix},
    \begin{bmatrix}
        \mathbf{K} & \mathbf{K}_* \\
        \mathbf{K}_*^T & \mathbf{K}_{* *}
    \end{bmatrix}\right),
\end{equation*}
where 
\begin{align*}
    \mathbf{m} &= (m(\mathbf{x}_1), \ldots, m(\mathbf{x}_N)), \\
    \mathbf{m_*} &= (m(\mathbf{v}_1), \ldots, m(\mathbf{v}_{N_{*}})), \\
    (\mathbf{K})_{ij} &= \kappa(\mathbf{x}_i, \mathbf{x}_j), \\
    (\mathbf{K}_*)_{ij} &= \kappa(\mathbf{x}_i, \mathbf{v}_j), \\
    (\mathbf{K}_{* *})_{ij} &= \kappa(\mathbf{v}_i, \mathbf{v}_j).
\end{align*}

Since Gaussian Processes are consistent by Theorem~\ref{thm:gp-const}, it follows from Theorem~\ref{thm:ogag} that our posterior distribution
of $(f(\mathbf{v}_1), \ldots, f(\mathbf{v}_{N_*}))$ given our observations is Gaussian with mean
\begin{equation} \label{eq:noisless-post-mean}
    \mathbf{m}_* + \mathbf{K}_*^T \mathbf{K}^{-1} (\mathbf{y} - \mathbf{m}), 
\end{equation}
and variance
\begin{equation} \label{eq:noiseless-post-var}
    \mathbf{K}_{* *} - \mathbf{K_*}^{T} \mathbf{K}^{-1} \mathbf{K_*}.
\end{equation}

One issue with applying Equations~\ref{eq:noisless-post-mean} and \ref{eq:noiseless-post-var} is that we assume noiseless and unbiased observations of $f$.
This is solemn the case in real-world experiments.
%For example if we bake two cakes with the same recipe twice, it is highly unlikely that we will result in the exact same cake due to a noisy process.
%This is problematic because later on we will see that a GP with an RBF kernel cannot represent data with two different observations of $f(\mathbf{x})$.
As we can see in Figure ~\ref{subfig:noiseless-post}, not accounting for these factors can cause our model to overfit and give us extreme results.

To incorporate the effects of bias and noise into our model, suppose that we have $N$ observations, $N_b$ observers, and $z_i$ indicates the observer for the $i$th sample.
Then, we can model the $i$th observation as a sum of the underlying function, the observer bias, and some noise as follows
\begin{equation*}
    y_i = f(\mathbf{x}_i) + b_{z_i} + \epsilon_i.
\end{equation*}
If we know $\mathbf{z} = (z_1, \ldots, z_n)$ 
and have the following priors
\begin{align*}
    \epsilon_i \sim_{iid} \mathcal{N}(0, \sigma^2_{\epsilon}) \text{ for $i = 1 \ldots, N$}\\
    b_j \sim_{iid} \mathcal{N}(0, \sigma_{b}^2) \text{ for $j = 1 \ldots, N_b$}
\end{align*}
and assume that that $f, \epsilon_1, \ldots, \epsilon_N, b_1, \ldots, b_{N_b}$ are independent,
then $\mathbf{y} = (y_1, \ldots, y_N)$ is a linear combination of Gaussian random variables
and is Gaussian by Theorem~\ref{thm:ogag}.

Despite these changes, posterior inference is similar.
Suppose $\mathbf{y} = (y_1, \ldots, y_N)$ are some noisy and biased observations of $(f(\mathbf{x}_1), \ldots, f(\mathbf{x}_n))$
and we would like to perform inference on $\mathbf{f}_* = (f(\mathbf{v}_1), \ldots, f(\mathbf{v}_{N_{*}}))$.
Our observations have the same prior mean of $\mathbf{m} = (m(\mathbf{x}_1), \ldots, m(\mathbf{x}_n))$ as before because $\epsilon_i$ and $b_j$ have mean 0 for all $i$ and $j$.
the covariance of $y_i$ and $y_{j}$ is
\begin{align*}
    (\mathbf{\Sigma})_{ij}
    &= \mathbb{E}[(y_i - m(\mathbf{x}_i) - 0 - 0)(y_j - m(\mathbf{x}_j - 0 - 0)] \\
    &= \mathbb{E}[(f(\mathbf{x}_i) + b_{z_i} + \epsilon_i - m(\mathbf{x}_i))(f(\mathbf{x}_j) + b_{z_i} + \epsilon_j - m(\mathbf{x}_j)] \\
    &= \kappa(\mathbf{x}_i, \mathbf{x}_{j}) + \mathbb{I}[z_i = z_{j}] \sigma^2_b + \mathbb{I}[i = j]\sigma^2_{ \epsilon }
\end{align*}
%and the covariance of $\mathbf{y}$ is
%\begin{equation*}
%    \bsy{\Sigma} = \mathbf{K} + \sigma^{2}_{\epsilon} \mathbf{I} + \sigma_b^2 \begin{bmatrix}
%        \mathbf{e}_{z_1} & \hdots & \mathbf{e}_{z_n}
%    \end{bmatrix}^{ T}
%     \begin{bmatrix}
%        \mathbf{e}_{z_1} & \hdots & \mathbf{e}_{z_n}
%    \end{bmatrix}
%\end{equation*}
%where $\mathbf{e}_i \in \mathbb{R}^{N}$ is the $i$th standard Euclidean basis vector.
The covariance between $y_i$ and $f(\mathbf{v}_j)$ is
\begin{align*}
    (\mathbf{K}_{*})_{ij}
    &= \mathbb{E}[ (y_i - m(\mathbf{x}_i))(f(\mathbf{v}_j) - m(\mathbf{v}_j)) \\
    &= \mathbb{E}[ (f(\mathbf{x}_i) + b_{z_i} + \epsilon_i - m(\mathbf{x}_i))(f(\mathbf{v}_j) - m(\mathbf{v}_j)) \\
    &= \mathbb{E}[ (f(\mathbf{x}_i) - m(\mathbf{x}_i))(f(\mathbf{v}_j) - m(\mathbf{v}_j)) \\
    &= \kappa(\mathbf{x}_i, \mathbf{v}_j).
\end{align*}
The covariance between $f(\mathbf{v}_i)$ and $f(\mathbf{v}_j)$ is
\begin{equation*}
    (\mathbf{K}_{* *})_{ij} = \kappa(\mathbf{v}_i, \mathbf{v}_j).
\end{equation*}
Finally, the prior mean of $\mathbf{f}_*$ is 
\begin{equation*}
    \mathbf{m}_* = (m(\mathbf{v}_1), \ldots, m(\mathbf{v}_{N_*})).
\end{equation*}
Since
\begin{equation*}
    \begin{bmatrix}
        \mathbf{y} \\ \mathbf{f}_{*}
    \end{bmatrix}
    =
    \begin{bmatrix}
        y_1 \\
        \vdots \\
        y_n \\
        f(\mathbf{v}_1) \\
        \vdots \\
        f(\mathbf{v}_{N_{*}}) \\
    \end{bmatrix}
    =
    \begin{bmatrix}
        f(\mathbf{x}_1) + b_{z_1} + \epsilon_1 \\
        \vdots \\
        f(\mathbf{x}_n) + b_{z_n} + \epsilon_n \\
        f(\mathbf{x}_*) \\
        \vdots \\
        f(\mathbf{v}_{N_{*}}) \\
    \end{bmatrix}
\end{equation*}
is a linear function of jointly random Gaussians $f(\mathbf{x}_1), \ldots, f(\mathbf{x}_n), f(\mathbf{v}_1), \ldots f(\mathbf{v}_{N_*}), \epsilon_1, \ldots, \epsilon_n, b_1 \ldots, b_{N_b}$,
the random vector $(\mathbf{y}, \mathbf{f}_*)$ is jointly Gaussian
\begin{equation*}
    \begin{bmatrix}
        \mathbf{y} \\
        \mathbf{f}_*
    \end{bmatrix}
    \sim
    \mathcal{N}\left(
    \begin{bmatrix}
            \mathbf{m} \\
            \mathbf{m}_*
        \end{bmatrix}
    ,
    \begin{bmatrix}
            \bsy{\Sigma} & \mathbf{K}_* \\
            \mathbf{K}_*^T & \mathbf{K}_{* *}
        \end{bmatrix}
    \right)
\end{equation*}
by Theorem~\ref{thm:ogag}.
The posterior of $\mathbf{f}_*$ given our observations is Gaussian with mean
\begin{equation*}
    \mathbf{m}_* + \mathbf{K}_*^T \bsy{\Sigma}^{-1} (\mathbf{y} - \mathbf{m})
\end{equation*}
and variance
\begin{equation*}
    \mathbf{K}_{* *} - \mathbf{K}_*^T \bsy{\Sigma}^{-1} \mathbf{K}_*.
\end{equation*}
These equations are similar to Equations \ref{eq:noisless-post-mean} and \ref{eq:noiseless-post-var},
but replacing $\mathbf{K}$ for $\bsy{\Sigma}$.
This difference is crucial because $\mathbf{K}$ might be singular
while $\bsy{\Sigma}$ is will be strictly positive definite when $\sigma_{\epsilon}^2 > 0$ because for any nonzero $\mathbf{c}$,
\begin{align*}
    \mathbf{c}^{T} \bsy{\Sigma} \mathbf{c}
    = \mathbf{c}^{T} \mathbf{K} \mathbf{c}
    + \mathbf{c}^{T} (\sigma^2_e \mathbf{I}) \mathbf{c} 
    +
    \mathbf{c}^{T}
    \begin{bmatrix}
        \mathbf{e}_{z_1} & \hdots & \mathbf{e}_{z_n}
    \end{bmatrix}^{ T}
     \begin{bmatrix}
        \mathbf{e}_{z_1} & \hdots & \mathbf{e}_{z_n}
    \end{bmatrix}
    \mathbf{c} \geq \mathbf{c}^{T} (\sigma^2_e \mathbf{I}) \mathbf{c}
    > 0.
\end{align*}


Figure~\ref{fig:gp-posteriors} shows the effect of incorporating noisy and biased observations.
expand...

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/noiseless-posterior.png}
        \caption{}
        \label{subfig:noiseless-post}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/noisy-posterior.png}
        \caption{}
        \label{subfig:noisy-posterior}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/biased-posterior.png}
        \caption{}
        \label{subfig:biased-posterior}
    \end{subfigure}
    \hfill
    \caption{Posteriors of a Gaussian Process given some noisy biased data.
        RBF kernel parameters are $\ell^{2} = 0.1^2, \sigma^2_{f} = 0.3 ^ 2, \sigma^2_b = 1^2$.
        Blue points have a bias of -1 while orange points have a bias of +1.
        Shaded area is 95\% posterior credible interval.
    }
    \label{fig:gp-posteriors}
\end{figure}
