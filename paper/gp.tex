\subsection{Gaussian Processes}

Gaussian Processes are an extension of Gaussian distributions to random continuous functions $f: \mathcal{X} \to \mathbb{R}$.
That is, for each element $\mathbf{x} \in \mathcal{X}$, we have a random variable $f(\mathbf{x})$.
If $\mathcal{X}$ is infinitely, working with Gaussian Processes seems overwhelming because we might try to create a joint distribution over all our random variables.
The key to making this problem tractable is to $f$ at a finite number of points.

\begin{definition}[Gaussian Process]\label{def:gp}
    A Gaussian Process on $\mathcal{X}$ is a random function $f$ parameterized by a mean function
    $m: \mathcal{X} \to \mathbb{R}$ and covariance/kernel function $\kappa: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$
    such that
    for any ($\mathbf{x}_1, \dots,\mathbf{x}_N) \subset \mathcal{X}$,
    \begin{equation*}
        \mathbf{f} \sim \mathcal{N}_N\left(\mathbf{m}, \mathbf{K}\right)
    \end{equation*}
    where
    \begin{equation*}
        \mathbf{f} = (f(\mathbf{x}_1), \dots, f(\mathbf{x}_n)), \mathbf{m} = (m(\mathbf{x}_1), \dots, m(\mathbf{x}_n)), (\mathbf{K})_{ij} = \kappa(\mathbf{x}_i, \mathbf{x}_j)
    \end{equation*}

    For this equation to make sense, we require $\kappa$ to generate positive semi-definite $\mathbf{K}$.
    We denote this as $f \sim \mathcal{GP}_{\mathcal{X}}(m, \kappa)$.
\end{definition}

We see that there is a one-to-one correspondence between the parameters of a Multivariate Gaussian $\mathcal{N}_{K}(\bsy{\mu}, \bsy{\Sigma})$ and that of a Gaussian process $\mathcal{GP}_{ \mathcal{X}}(m, \kappa)$.
The set $\mathcal{X}$ correspondes to $K$ as they both represent the amount of random variables.
The function $m$ corresponds with $\bsy{\mu}$ in that they both give us the means of our random variables.
The kernel $\kappa$ corresponds with $\bsy{\Sigma}$ as they both tell us the covariance between random variables.
They two distribution are also similar in that they have the property that marginals of a finite number of random variables is a Gaussian Distribution.
The key difference is that with Gaussians Distributions, this property of finite marginals is a \emph{result} derived from the full joint distribution while Gaussian Processes are \emph{defined} by this property.

\subsubsection{Consistency of Gaussian Processes}

In Definition~\ref{def:gp}, we asserted that finite marginals are Gaussian, but we still need to verify that these marginals are consistent with each other.
For example, given random variables $X, Y, Z$, it make no sense to say that the marginals are
\begin{align*}
    (X, Y) = (0, 0) \text{ with probability 1} \\
    (Y, Z) = (1, 1) \text{ with probability 1}
\end{align*}
because then $Y$ must equal both 0 and 1.
For the marginals from Definition~\ref{def:gp} to be consistent, we must prove the existence of a probability space and random variables that generates such marginals.
The Kolmogorov Extension Theorem helps do just this

\begin{theorem}[Kolmogorov Extension Theorem]\label{thm:kol-ext}
    Suppose that for some set $\mathcal{X}$, we have an indexed collection of functions
    \begin{equation*}
        M = \{ \mu_{\mathbf{x}_1, \dots, \mathbf{x}_N} : \mathcal{B}_{N} \to \mathbb{R} \;|\; N \in \mathbb{N} \text{ and } (\mathbf{x}_1, \dots, \mathbf{x}_N) \subseteq \mathcal{X}\}.
    \end{equation*}
    such that for all $N \in \mathbb{N}, (\mathbf{x}_1, \dots, \mathbf{x}_N) \subseteq \mathcal{X}$,
    the following conditions hold:
    \begin{enumerate}
        \item $\mu_{\mathbf{x}_1, \dots, \mathbf{x}_N}( \varnothing) = 0$,

        \item $\mu_{\mathbf{x}_1, \dots, \mathbf{x}_N}(\mathbb{R}^{N}) = 1$,
        \item for every disjoint sequence of sets $(E_1, E_2, \dots) \subseteq \mathcal{B}_N$,
            \begin{equation*}
                \mu_{\mathbf{x}_1, \dots, \mathbf{x}_N}\left(\bigcup_{i = 1}^{\infty}
                E\right) = \sum_{i = 1}^{\infty}\mu_{\mathbf{x}_1, \dots, \mathbf{x}_N}(E_i).
            \end{equation*}
    \end{enumerate}
    In other words, $\mu_{\mathbf{x}_1, \dots, \mathbf{x}_N}$ is a probability measure on $\mathbb{R}^{N}$.

    There exists a probability space $(\Omega, \mathcal{F}, P)$ and random variables $\{ f(\mathbf{x} | \mathbf{x} \in \mathcal{X} \}$ such that
    \begin{equation*}
        P((f(\mathbf{x}_1), \dots, f(\mathbf{x}_N)) \in E) = \mu_{\mathbf{x}_1, \dots, \mathbf{x}_N}(E)
    \end{equation*}
    %for any $N \in \mathbb{N}$, $(\mathbf{x}_1, \dots, \mathbf{x}_N) \subseteq \mathcal{X}, E \in \mathcal{B}_{N}$, 
    if the following two conditions hold for all $N \in \mathbb{N}, (\mathbf{x}_1, \dots, \mathbf{x}_N) \subseteq \mathcal{X}$:
    \begin{enumerate}
        \item For any permutation $\pi$ of $\{ 1, \dots, N \}$ and Borel sets $E_1, \dots, E_N$,
            \begin{equation*}
                \mu_{\mathbf{x}_1, \dots, \mathbf{x}_N}(E_1 \times \dots \times E_N)
                =\mu_{\mathbf{x}_{\pi(1)}, \dots, \mathbf{x}_{\pi(N)}}\left(E_{\pi(1)} \times \dots \times E_{\pi(N)}\right)
            \end{equation*}
        \item For any $\mathbf{x} \in \mathcal{X}$,
            \begin{equation*}
                \mu_{\mathbf{x}_1, \dots, \mathbf{x}_N}(E_1 \times \dots \times E_N)
                =
                \mu_{\mathbf{x}_1, \dots, \mathbf{x}_N, \mathbf{x}}(E_1 \times \dots \times E_N \times \mathbb{R})
            \end{equation*}
    \end{enumerate}
\end{theorem}

\begin{theorem}
    \label{thm:gp-const}
    Gaussian Processes are consistent
\end{theorem}
\begin{proof}
    Let $\mathcal{N}( \cdot | \mathbf{m}, \mathbf{K})$
    be the cdf of a Gaussian Distribution with mean $\mathbf{m}$ and covariance $\mathbf{K}$ as in Definition~\ref{def:gp}:
    \begin{equation*}
        \mu_{\mathbf{x}_1, \ldots, \mathbf{x}_N}(E)
        = \int_{E} \mathcal{N}( \mathbf{y} | \mathbf{m}, \mathbf{K}) d \lambda_N(\mathbf{y})
    \end{equation*}
    where $\mathbf{m} = (m(\mathbf{x}_1), \ldots, m(\mathbf{x}_N))$,
    $(\mathbf{K})_{ij} = \kappa(\mathbf{x}_i, \mathbf{x}_j)$, and
    $\lambda_N$ is the Lebesgue measure on $\mathbb{R}^{N}$ (Definition 5.40 of \cite{axler2020}).

    First, we show that $\mu_{\mathbf{x}_1, \ldots, \mathbf{x}_N}$ is a probability measure.
    Since $\varnothing$ has measure 0,
    \begin{equation*}
        \int_{\varnothing} \mathcal{N}(\mathbf{y} | \mathbf{m}, \mathbf{K}) d \lambda_N(\mathbf{y}) = 0
    \end{equation*}
    Further, because $\mathcal{N}( \cdot | \mathbf{m}, \mathbf{K})$ is a valid density function,
    \begin{equation*}
        \int_{\mathbb{R}^{N}} \mathcal{N}(\mathbf{y} | \mathbf{m}, \mathbf{K}) d \lambda_N(\mathbf{y}) = 1
    \end{equation*}

    Now, let $(E_1, E_2, \ldots) \subseteq \mathcal{B}_{N}$ be a sequence of disjoint sets, $E = E_1 \cup E_2 \cup \ldots$, and
    \begin{equation*}
        \chi_{E}(\mathbf{y}) =
        \begin{cases}
            1 & \text{ if $\mathbf{y} \in E$} \\
            0 & \text{ if $\mathbf{y} \notin E$}.
        \end{cases}
    \end{equation*}
    For some $K \in \mathcal{N}$, we can write
    \begin{align*}
        \sum_{i = 1}^{K} \mu_{\mathbf{x}_1, \ldots, \mathbf{x}_N}(E_i)
        & = \sum_{i = 1}^{K} \int_{E_i} \mathcal{N}(\mathbf{y} | \mathbf{m}, \mathbf{K}) d \lambda_N(\mathbf{y}) \\
        & = \sum_{i = 1}^{K} \int_{\mathbb{R}^{N}} \chi_{E_i}(\mathbf{y})\mathcal{N}(\mathbf{y} | \mathbf{m}, \mathbf{K}) d \lambda_N(\mathbf{y}) \\
        & =  \int_{\mathbb{R}^{N}} \sum_{i = 1}^{K}\chi_{E_i}(\mathbf{y})\mathcal{N}(\mathbf{y} | \mathbf{m}, \mathbf{K}) d \lambda_N(\mathbf{y}) \\
        & =  \int_{\mathbb{R}^{N}} \chi_{E_1 \cup \ldots \cup E_K}(\mathbf{y})\mathcal{N}(\mathbf{y} | \mathbf{m}, \mathbf{K}) d \lambda_N(\mathbf{y}) \\
    \end{align*}
    The last equality comes from the fact that $E_1, \ldots, E_K$ are disjoint.

    We see that $\chi_{E_1 \cup \ldots E_K}(\cdot)\mathcal{N}(\cdot | \mathbf{m}, \mathbf{K})$ converges to $\chi_E(\cdot) \mathcal{N}(\cdot | \mathbf{m}, \mathbf{K})$ pointwise.
    Further for any $\mathbf{y}$ and $K$,
    \begin{equation*}
        \chi_{E_1 \cup \ldots \cup E_K}(\mathbf{y}) \mathcal{N}(\mathbf{y} | \mathbf{m}, \mathbf{K})
        \leq
        \chi_E(\mathbf{y}) \mathcal{N}(\mathbf{y} | \mathbf{m}, \mathbf{K})
        \leq \max_{\mathbf{y}'}\mathcal{N}(\mathbf{y'} | \mathbf{m}, \mathbf{K})
        < \infty.
    \end{equation*}
    Applying the Monotone Convergence Theorem,
    \begin{align*}
        \sum_{i = 1}^{\infty} \mu_{\mathbf{x}_1, \ldots, \mathbf{x}_N}(E_i)
        & = \lim_{K \to \infty}\int_{\mathbb{R}^{N}} \chi_{E_1 \cup \ldots \cup E_K}(\mathbf{y})\mathcal{N}(\mathbf{y} | \mathbf{m}, \mathbf{K}) d \lambda_N(\mathbf{y}) \\
        & = \int_{\mathbb{R}^{N}} \chi_{K}(\mathbf{y})\mathcal{N}(\mathbf{y} | \mathbf{m}, \mathbf{K}) d \lambda_N(\mathbf{y}) \\
        & = \mu_{\mathbf{x}_1, \ldots, \mathbf{x}_N}(E).
    \end{align*}
    Thus, $\mu_{\mathbf{x}_1, \ldots, \mathbf{x}_n}$ is a probability measure.

    Now, we show that consistency conditions hold.
    We assume that Let $\pi$ be a permutation of $\{ 1, \ldots, N \}$.
    Then, there exists a permutation matrix $\mathbf{P} \in \mathbb{R}^{N \mathbf{x} N}$
    such that
    \begin{equation*}
        \mu_{\mathbf{x}_{\pi(1)}, \ldots, \mathbf{x}_{\pi(N)}}(E) =
        \int_{E} \mathcal{N}(\mathbf{y} | \mathbf{Pm}, \mathbf{P} \bsy{\Sigma} \mathbf{P}^{T}) d \lambda_N(\mathbf{y}).
    \end{equation*}
    and if $E_1, \ldots, E_N$ are Borel, then using a change of variables $\mathbf{w} = \mathbf{Py}$
    \begin{align*}
        & \mu_{\mathbf{x}_1, \ldots, \mathbf{x}_N}(E_1 \times \ldots \times E_N) \\
        & = \int_{E_1 \times \ldots \times E_N} \mathcal{N}(\mathbf{y} | \mathbf{m}, \mathbf{K}) d \lambda_N(\mathbf{y}) \\
        & = \int_{E_1 \times \ldots \times E_N}
        \frac{ 1 }{ (2 \pi)^{N/2} \lvert \mathbf{K} \rvert^{1/2} }
        \exp \{ (\mathbf{y} - \mathbf{m})^{T} \Sigma^{-1} (\mathbf{y} - \mathbf{m}) \}
        \lambda_N(\mathbf{y}) \\
        & = \int_{E_{\pi(1)} \times \ldots \times E_{\pi(N)}}
        \frac{ 1 }{ (2 \pi)^{N/2} \lvert \mathbf{PKP^T} \rvert^{1/2} }
        \exp \{ (\mathbf{P^Tw} - \mathbf{m})^{T} \Sigma^{-1} (\mathbf{P}^{T}\mathbf{y} - \mathbf{m}) \}
        \lvert \mathbf{P}^{T} \rvert \lambda_N(\mathbf{w}) \\
        & = \int_{E_{\pi(1)} \times \ldots \times E_{\pi(N)}}
        \frac{ 1 }{ (2 \pi)^{N/2} \lvert \mathbf{PKP}^T \rvert^{1/2} }
        \exp \{ (\mathbf{y} - \mathbf{Pm})^{T} (\mathbf{P}\bsy{\Sigma} \mathbf{P})^{-1} (\mathbf{y} - \mathbf{P}\mathbf{m}) \} \lambda_N(\mathbf{w}) \\
        & = \int_{E_{\pi(1)} \times \ldots \times E_{\pi(N)}}
        \mathcal{N}(\mathbf{w} | \mathbf{Pm}, \mathbf{P} \bsy{\Sigma} \mathbf{P}^{T}) d \lambda(\mathbf{w}) \\
        &= \mu_{\mathbf{x}_{\pi(1)}, \ldots, \mathbf{x}_{\pi(N)}}
    \end{align*}
    Note that $\mathbf{P}^{T} = \mathbf{P}^{-1}$ and $\lvert \mathbf{P} \rvert = 1$>

    The second consistency condition follows directly from (a) of \ref{thm:ogag}.

    Since both conditions are met, Gaussian Processes are consistent.
\end{proof}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/gp-sample2d.png}
        \caption{}
        \label{subfig:2d-gp-sample}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/gp-sample3d.png}
        \caption{}
        \label{subfig:3d-gp-sample}
    \end{subfigure}
    \hfill
    \caption{Samples of a Gaussian Process using a RBF kernel with parameters $\ell^{2} = 0.1^2$ and $\sigma^{2}_f = 1 ^ 2$ with
    $\mathcal{X} = [0, 1]$ (a) and $\mathcal{X} = [0, 1]^2$ (b).
    }
    \label{fig:gp-sample}
\end{figure}

\subsubsection{Continuity of Gaussian Processes}

Figure \ref{fig:gp-sample} demonstrates samples from Gaussian Processes where $\mathcal{X}$ is 1 or 2-dimensional.
In both cases we use a kernel such based on Euclidean distance and see that $f(\mathbf{x})$ and $f(\mathbf{x}')$ are close when $\mathbf{x}$ and $\mathbf{x}'$ are close.
In other words, samples of $f$ are continuous.
But because $f$ is a collection of random variables, we first need to formalize what it means for distribution to converge.

\begin{definition}[Converges in Distribution]\label{def:cvg-dst}
    A sequence of random variables $X_n$ is said to converge in distribution to $X$
    if for any bounded continuous function $h$,
    \begin{equation*}
        \lim_{n\to \infty} \mathbb{E}[h(X_n)] = \mathbb{E}[h(X)].
    \end{equation*}
\end{definition}

With this definition in place, we can show that as $\mathbf{x}_n$ approaches $\mathbf{x}$, $f(\mathbf{x}) - f(\mathbf{x}')$ will approach 0 in distribution.
\begin{theorem}
    Let $f \sim \mathcal{GP}(m, \kappa)$.
    If $m$ and $\kappa$ are continuous, then for any convergent sequence $(\mathbf{x}_n) \to \mathbf{x}$, $(f(\mathbf{x}_n) - f(\mathbf{x}))$ converges to a distribution to a degenerate distribution at 0.
\end{theorem}
\begin{proof}
    Suppose that $(\mathbf{x}_n) \to \mathbf{x}$, and that $m, \kappa$ are continuous functions.
    Let the mean and variance $f(\mathbf{x}_n) - f(\mathbf{x})$ be
    \begin{align*}
        \mu_n & = m(\mathbf{x}_n) - m(\mathbf{x}) \\
        \sigma_n^2 & = \kappa(\mathbf{x}, \mathbf{x}) + \kappa(\mathbf{x}_n, \mathbf{x}_n) - 2\kappa(\mathbf{x}, \mathbf{x}_n).
    \end{align*}
    It follows that $f(\mathbf{x}_n) -f(\mathbf{x}) \sim \mathcal{N}(\mu_n, \sigma^2_n)$.
    Also, let $Y$ be a random variable that always equals 0.
    We need to show that
    \begin{equation*}
        \lim_{n \to \infty}\mathbb{E}_{X \sim \mathcal{N}(\mu_n, \sigma^2)}[h(X)] = \mathbb{E}[h(Y)] = h(0)
    \end{equation*}
    which is equivalent to showing that
    \begin{equation*}
        \lim_{n \to \infty} \lvert \mathbb{E}_{X \sim \mathcal{N}(\mu_n, \sigma_n^2)}[h(X) - h(0)] \rvert = 0.
    \end{equation*}
    First, we reparametrizations the left hand side to make it easier to work with.
    Let $U \sim \mathcal{N}(0, 1)$.
    Then,
    \begin{align*}
        & \lim_{n \to \infty} \lvert \mathbb{E}_{X \sim \mathcal{N}(\mu_n, \sigma_n^2)}[h(X) - h(0)] \rvert \\
        & =
        \lim_{n \to \infty} \lvert \mathbb{E}[h(\mu_n + \sigma_n U) - h(0)] \rvert \\
        & \leq
        \lim_{n \to \infty}
        \lvert \mathbb{E}[h(\mu_n + \sigma_n U) - h(\sigma_n U)] \rvert
        +
        \lvert \mathbb{E}[h(\sigma_nU) - h(0)] \rvert \\
    \end{align*}

    First, we consider the term

    \begin{align*}
        \lvert \mathbb{E}[h(\mu_n + \sigma_n U) - h(\sigma_n U)] \rvert
        & \leq
        \int_{\mathbb{R}} \lvert h(\mu_n + \sigma_n u) - h(\sigma_n u) \rvert \phi(u) du
    \end{align*}
    Because $h$ and $\phi$ are continuous, we know that $\lvert h(\mu_n + \sigma_n u) - h(\sigma_n u) \rvert$ converges to the 0 function pointwise as a function $u$.
    Further, because $h$ and $\phi$ are bounded, there exists an $M'$ such that $\lvert h(\mu_n + \sigma_n u) - h(\sigma_n u) \rvert \phi(u) < M$ for all $u$ and $n$.
    Thus, by the Monotone Convergence Theorem (Theorem 3.11 of \cite{axler2020}),
    \begin{align*}
        \lim_{n \to \infty} \lvert \mathbb{E}[h(\mu_n + \sigma_n U) - h(\sigma_n U)] \rvert
        & = \lim_{n\to\infty} \int_{\mathbb{R}} \lvert h(\mu_n + \sigma_n u) - h(\sigma_n u) \rvert \phi(u) du \\
        & = \lim_{n\to\infty} \int_{\mathbb{R}} \lvert h(\mu_n + \sigma_n u) - h(\sigma_n u) \rvert \phi(u) du \\
        & =  \int_{\mathbb{R}} 0 du \\
        & = 0.
    \end{align*}

    Now, let $\epsilon > 0$, $f_n$ be the pdf of $V_n = \sigma$ and $\Phi$ be the cdf of $U$.
    Because $h$ is continuous, there exists a $\delta > 0$ such that
    $ \lvert v - 0 \rvert < \delta $ implies $h(v) - h(0) \leq \epsilon / 2$
    \begin{align*}
        & \lvert \mathbb{E}[h(\sigma_n U) - h(0)] \rvert \\
        & \leq
        \int_{\mathbb{R}} \lvert h(\sigma_n u) - h(0) \phi(u) \lvert du \\
        %&= \int_{(-\delta / \sigma_n, \delta / \sigma_n)} \lvert h(\sigma_n u) - h(0) \lvert \phi(u) du
        %+ \int_{(-\infty, -\delta / \sigma_n)} \lvert h(\sigma_n u) - h(0) \lvert \phi(u) du
        %+ \int_{(\delta / \sigma_n, \infty)} \lvert h(\sigma_n u) - h(0) \lvert \phi(u) du \\
        & \leq
        \int_{(-\delta / \sigma_n, \delta / \sigma_n)} \lvert h(\sigma_n u) - h(0) \rvert \phi(u) du
        + \int_{(-\infty, -\delta / \sigma_n)}
        M \phi(u) du + \int_{(\delta / \sigma_n, \infty)} M \phi(u) du \\
        & \leq \int_{(-\delta / \sigma_n, \delta / \sigma_n)} \lvert \frac{\epsilon}2\rvert \phi(u) du + 2M \Phi(-\frac{ \delta }{ \sigma_n }) \\
        & \leq \int_{\mathbb{R}} \lvert \frac{\epsilon}2\rvert \phi(u) du + 2M \Phi(-\frac{ \delta }{ \sigma_n }) \\
        & \leq \frac\epsilon2 + 2M \Phi(-\frac{ \delta }{ \sigma_n })
    \end{align*}
    Becuase $\Phi$ is continuous, and $\lim_{u \to -\infty}\Phi(u) = 0$, there exists an $N$ such that for all $n > N$, $\Phi(\delta/\sigma_n) < \epsilon / (4M)$.
    Thus for $n > N$,
    \begin{align*}
        \lvert \mathbb{E}[h(\sigma_n U) - h(0)] \rvert
        & \leq \frac\epsilon2
        + 2M \Phi(-\frac{ \delta }{ \sigma_n })
        \leq \epsilon
    \end{align*}
    and
    \begin{equation*}
        \lim_{n \to \infty} \lvert \mathbb{E}[h(\sigma_n U) - h(0)] \rvert = 0
    \end{equation*}

    Since both
    \begin{align*}
        & \lvert \mathbb{E}[h(\mu_n + \sigma_n U) - h(\sigma_n U)] \rvert \\
        & \lvert \mathbb{E}[h(\sigma_nU) - h(0)] \rvert
    \end{align*}
    approach 0 as $n$ approaches $\infty$,
    \begin{equation*}
        \lim_{n \to \infty}  \mathbb{E}[h(f(\mathbf{x}_n) - f(\mathbf{x})) - h(0)]  = 0
    \end{equation*}
    and
    $f(\mathbf{x}_n) - f(\mathbf{x})$ converges to a degenerate distribution at 0.
\end{proof}

\subsubsection{Posterior Inference on GP}

We now turn our attention to posterior inference.
That is, we want to quantify our belief of $f$ given some observations.
Suppose we observe that
\begin{equation*}
    \mathbf{f} = (f(\mathbf{x}_1), \dots f(\mathbf{x}_N)) = \mathbf{y} = (y_1, \dots, y_N).
\end{equation*}

Then, for some points of interest $\mathbf{v}_{1}, \dots, \mathbf{v}_{N_{*}}$, the joint distribution of $\mathbf{f}$ and $\mathbf{f}_* = (f(\mathbf{v}_1), \dots, f(\mathbf{v}_{N_*}))$ is
\begin{equation*}
    \begin{bmatrix}
        \mathbf{f} \\
        \mathbf{f}_{*} \\
    \end{bmatrix}
    \sim
    \mathcal{N}\left(
    \begin{bmatrix}
            \mathbf{m} \\
            \mathbf{m}_{*}
        \end{bmatrix}
    ,
    \begin{bmatrix}
            \mathbf{K} & \mathbf{K}_* \\
            \mathbf{K}_*^T & \mathbf{K}_{* *}
        \end{bmatrix}
    \right),
\end{equation*}
where
\begin{align*}
    \mathbf{m} & = (m(\mathbf{x}_1), \dots, m(\mathbf{x}_N)), \\
    \mathbf{m_*} & = (m(\mathbf{v}_1), \dots, m(\mathbf{v}_{N_{*}})), \\
    (\mathbf{K})_{ij} & = \kappa(\mathbf{x}_i, \mathbf{x}_j), \\
    (\mathbf{K}_*)_{ij} & = \kappa(\mathbf{x}_i, \mathbf{v}_j), \\
    (\mathbf{K}_{* *})_{ij} & = \kappa(\mathbf{v}_i, \mathbf{v}_j).
\end{align*}

Since Gaussian Processes are consistent by Theorem~\ref{thm:gp-const}, it follows from Theorem~\ref{thm:ogag} that the posterior
of $(f(\mathbf{v}_1), \dots, f(\mathbf{v}_{N_*}))$ given our observations is Gaussian with mean
\begin{equation}
    \label{eq:noisless-post-mean}
    \mathbf{m}_* + \mathbf{K}_*^T \mathbf{K}^{-1} (\mathbf{y} - \mathbf{m}),
\end{equation}
and variance
\begin{equation}
    \label{eq:noiseless-post-var}
    \mathbf{K}_{* *} - \mathbf{K_*}^{T} \mathbf{K}^{-1} \mathbf{K_*}.
\end{equation}
when $\mathbf{K}$ is nonsingular.

One issue with applying Equations~\ref{eq:noisless-post-mean} and \ref{eq:noiseless-post-var} is that we assume noiseless and unbiased observations of $f$.
This is solemn the case with empirical observation.
As we can see in Figure~\ref{subfig:noiseless-post}, not accounting for these factors can cause our model to overfit and give us extreme results.

To incorporate the effects of bias and noise into our model, suppose that we have $N$ observations, $N_b$ observers numbered 1 through $N_b$, and $z_i$ indicates the observer for the $i$th sample.
Then, we can model the $i$th observation as a sum of the underlying function, the observer bias, and some noise
\begin{equation*}
    y_i = f(\mathbf{x}_i) + b_{z_i} + \epsilon_i.
\end{equation*}
If we know $\mathbf{z} = (z_1, \dots, z_n)$, and have Gaussian priors
\begin{align*}
    \epsilon_i \sim_{iid} \mathcal{N}(0, \sigma^2_{\epsilon}) \text{ for $i = 1 \dots, N$}, \\
    b_j \sim_{iid} \mathcal{N}(0, \sigma_{b}^2) \text{ for $j = 1 \dots, N_b$},
\end{align*}
and assume that that $f, \epsilon_1, \dots, \epsilon_N, b_1, \dots, b_{N_b}$ are independent,
then $\mathbf{y} = (y_1, \dots, y_N)$ is a linear combination of Gaussian random variables
and is Gaussian by Theorem~\ref{thm:ogag}.

Despite these changes, posterior inference is similar.
Suppose $\mathbf{y} = (y_1, \dots, y_N)$ are some noisy and biased observations of $(f(\mathbf{x}_1), \dots, f(\mathbf{x}_n))$ by observers $\mathbf{z} = (z_1, \ldots, z_N)$ respectively, and we would like to perform inference on $\mathbf{f}_* = (f(\mathbf{v}_1), \dots, f(\mathbf{v}_{N_{*}}))$.
Our observations $\mathbf{y}$ have the same prior mean of $\mathbf{m} = (m(\mathbf{x}_1), \dots, m(\mathbf{x}_n))$ as before because $\epsilon_i$ and $b_j$ have mean 0 for all $i$ and $j$.
The covariance of $y_i$ and $y_{j}$ is
\begin{align*}
    (\mathbf{\Sigma})_{ij}
    & = \mathbb{E}[(y_i - m(\mathbf{x}_i))(y_j - m(\mathbf{x}_j)] \\
    & = \mathbb{E}[(f(\mathbf{x}_i) + b_{z_i} + \epsilon_i - m(\mathbf{x}_i))(f(\mathbf{x}_j) + b_{z_i} + \epsilon_j - m(\mathbf{x}_j)] \\
    & = \kappa(\mathbf{x}_i, \mathbf{x}_{j}) + \mathbb{I}[z_i = z_{j}] \sigma^2_b + \mathbb{I}[i = j]\sigma^2_{ \epsilon }
\end{align*}
%and the covariance of $\mathbf{y}$ is
%\begin{equation*}
%    \bsy{\Sigma} = \mathbf{K} + \sigma^{2}_{\epsilon} \mathbf{I} + \sigma_b^2 \begin{bmatrix}
%        \mathbf{e}_{z_1} & \hdots & \mathbf{e}_{z_n}
%    \end{bmatrix}^{ T}
%     \begin{bmatrix}
%        \mathbf{e}_{z_1} & \hdots & \mathbf{e}_{z_n}
%    \end{bmatrix}
%\end{equation*}
%where $\mathbf{e}_i \in \mathbb{R}^{N}$ is the $i$th standard Euclidean basis vector.
The covariance between $y_i$ and $f(\mathbf{v}_j)$ is
\begin{align*}
    (\mathbf{K}_{*})_{ij}
    & = \mathbb{E}[ (y_i - m(\mathbf{x}_i))(f(\mathbf{v}_j) - m(\mathbf{v}_j)) \\
    & = \mathbb{E}[ (f(\mathbf{x}_i) + b_{z_i} + \epsilon_i - m(\mathbf{x}_i))(f(\mathbf{v}_j) - m(\mathbf{v}_j)) \\
    & = \mathbb{E}[ (f(\mathbf{x}_i) - m(\mathbf{x}_i))(f(\mathbf{v}_j) - m(\mathbf{v}_j)) \\
    & = \kappa(\mathbf{x}_i, \mathbf{v}_j).
\end{align*}
The covariance between $f(\mathbf{v}_i)$ and $f(\mathbf{v}_j)$ is
\begin{equation*}
    (\mathbf{K}_{* *})_{ij} = \kappa(\mathbf{v}_i, \mathbf{v}_j).
\end{equation*}
Finally, the prior mean of $\mathbf{f}_*$ is
\begin{equation*}
    \mathbf{m}_* = (m(\mathbf{v}_1), \dots, m(\mathbf{v}_{N_*})).
\end{equation*}
Since
\begin{equation*}
    \begin{bmatrix}
        \mathbf{y} \\
        \mathbf{f}_{*}
    \end{bmatrix}
    =
    \begin{bmatrix}
        y_1 \\
        \vdots \\
        y_n \\
        f(\mathbf{v}_1) \\
        \vdots \\
        f(\mathbf{v}_{N_{*}}) \\
    \end{bmatrix}
    =
    \begin{bmatrix}
        f(\mathbf{x}_1) + b_{z_1} + \epsilon_1 \\
        \vdots \\
        f(\mathbf{x}_n) + b_{z_n} + \epsilon_n \\
        f(\mathbf{x}_*) \\
        \vdots \\
        f(\mathbf{v}_{N_{*}}) \\
    \end{bmatrix}
\end{equation*}
is a linear function of jointly Gaussian random variables
\begin{equation*}
    f(\mathbf{x}_1), \dots, f(\mathbf{x}_n), f(\mathbf{v}_1), \dots f(\mathbf{v}_{N_*}), \epsilon_1, \dots, \epsilon_n, b_1 \dots, b_{N_b}
\end{equation*}
,
the random vector $(\mathbf{y}, \mathbf{f}_*)$ is Gaussian
\begin{equation*}
    \begin{bmatrix}
        \mathbf{y} \\
        \mathbf{f}_*
    \end{bmatrix}
    \sim
    \mathcal{N}\left(
    \begin{bmatrix}
            \mathbf{m} \\
            \mathbf{m}_*
        \end{bmatrix}
    ,
    \begin{bmatrix}
            \bsy{\Sigma} & \mathbf{K}_* \\
            \mathbf{K}_*^T & \mathbf{K}_{* *}
        \end{bmatrix}
    \right)
\end{equation*}
by Theorem~\ref{thm:ogag}.
The posterior of $\mathbf{f}_*$ given our observations is Gaussian with mean
\begin{equation*}
    \mathbf{m}_* + \mathbf{K}_*^T \bsy{\Sigma}^{-1} (\mathbf{y} - \mathbf{m})
\end{equation*}
and variance
\begin{equation*}
    \mathbf{K}_{* *} - \mathbf{K}_*^T \bsy{\Sigma}^{-1} \mathbf{K}_*.
\end{equation*}
These equations are similar to Equations \ref{eq:noisless-post-mean} and \ref{eq:noiseless-post-var}, but replacing $\mathbf{K}$ for $\bsy{\Sigma}$.
This difference is crucial as $\mathbf{K}$ might be singular
while $\bsy{\Sigma}$ is will be strictly positive definite when $\sigma_{\epsilon}^2 > 0$ because
\begin{align*}
    \mathbf{c}^{T} \bsy{\Sigma} \mathbf{c}
    = \mathbf{c}^{T} \mathbf{K} \mathbf{c}
    + \mathbf{c}^{T} (\sigma^2_e \mathbf{I}) \mathbf{c}
    +
    \mathbf{c}^{T}
    \begin{bmatrix}
        \mathbf{e}_{z_1} & \hdots & \mathbf{e}_{z_n}
    \end{bmatrix}
    ^{ T}
    \begin{bmatrix}
        \mathbf{e}_{z_1} & \hdots & \mathbf{e}_{z_n}
    \end{bmatrix}
    \mathbf{c} \geq \mathbf{c}^{T} (\sigma^2_e \mathbf{I}) \mathbf{c}
    > 0.
\end{align*}

Figure~\ref{fig:gp-posteriors} shows the effect of incorporating noisy and biased observations.
expand...

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/noiseless-posterior.png}
        \caption{}
        \label{subfig:noiseless-post}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/noisy-posterior.png}
        \caption{}
        \label{subfig:noisy-posterior}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/biased-posterior.png}
        \caption{}
        \label{subfig:biased-posterior}
    \end{subfigure}
    \hfill
    \caption{Posteriors of a Gaussian Process given some noisy biased data.
    RBF kernel parameters are $\ell^{2} = 0.1^2, \sigma^2_{f} = 0.3 ^ 2, \sigma^2_b = 1^2$.
    Blue points have a bias of -1 while orange points have a bias of +1.
    Shaded area is 95\% posterior credible interval.
    }
    \label{fig:gp-posteriors}
\end{figure}
