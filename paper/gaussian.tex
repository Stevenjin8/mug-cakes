\subsection{Gaussian Distributions}

This section reviews important properties of Gaussian distributions that will use to make posterior inference using observed data in our Bayesian Optimization loop.
We begin by defining Gaussian random variables.

\begin{definition}[Nondegenerate Gaussian]
    The probability density of a $K$-dimensional (Multivariate) Nondegenerate Gaussian random vector $\mathbf{x} \sim \mathcal{N}_K(\bsy{\mu}, \bsy{\Sigma})$
    is
    \begin{equation}
        \label{mvn-pdf}
        \frac{ 1 }{ (2 \pi)^{K/2} \lvert \bsy{\Sigma} \rvert^{1/2} } \exp \left\{ -\frac12 (\mathbf{x} - \bsy{\mu})^{T} \bsy{\Sigma}^{-1} (\mathbf{x} - \bsy{\mu})  \right\}
    \end{equation}
    where $\bsy{\mu} \in \mathbb{R}^{K}$ is the mean vector and $\bsy{\Sigma}$ is a $K \times K$ strictly positive definite covariance matrix.
    We just write $\mathcal{N}$ instead of $\mathcal{N}_K$ if $K$ is clear from context.
\end{definition}

We can also allow $\bsy{\Sigma}$ to be singular giving us a degenerate Gaussian, but these are much harder to work with because we rely on $\bsy{\Sigma}^{-1}$ in Equation~\ref{mvn-pdf}.

One reason why Gaussians are popular is because they stay Gaussian under various transformations, as the next theorem shows.

\begin{theorem}[Once Gaussian Always Gaussian]\label{thm:ogag}
    Suppose $\mathbf{x} = (\mathbf{x}_1, \mathbf{x}_2)$ be a $K$-dimensional multivariate Gaussian with mean and variance
    \begin{equation*}
        \bsy{\mu} =
        \begin{pmatrix}
            \bsy{\mu}_1 \\
            \bsy{\mu}_2
        \end{pmatrix}
        ,\quad
        \bsy{\Sigma} =
        \begin{pmatrix}
            \bsy{\Sigma}_{11} &  & \bsy{\Sigma}_{12} \\
            \bsy{\Sigma}_{21} &  & \bsy{\Sigma}_{22} \\
        \end{pmatrix}
    \end{equation*}
    where
    \begin{align*}
        \mathbf{x}_1, \bsy{\mu}_1 \in \mathbb{R}^{K_1};
        \mathbf{x}_2, \bsy{\mu}_2 \in \mathbb{R}^{K_2};
        \bsy{\Sigma}_{11} \in \mathbb{R}^{K_1 \times K_1};
        \bsy{\Sigma}_{22} \in \mathbb{R}^{K_2 \times K_2};
        \bsy{\Sigma}_{12} = \bsy{\Sigma}_{21}^{T} \in \mathbb{R}^{K_1 \times K_2}.
    \end{align*}

    Then, the following are true.
    \begin{enumerate}
        \item
            The marginal of $\mathbf{x}_1$ is
            \begin{equation*}
                \mathbf{x}_{1} \sim \mathcal{N}_{K_1}(\bsy{\mu}_1, \bsy{\Sigma}_{11}).
            \end{equation*}

        \item
            The conditional of $\mathbf{x}_1$ given $\mathbf{x}_2$ is Gaussian
            \begin{align*}
                \mathbf{x}_1 | \mathbf{x}_2 &\sim \mathcal{N}_{K_1}(\bsy{\mu}_{1 | 2}, \bsy{\Sigma}_{1 | 2}) \\
                \bsy{\mu}_{1 | 2} &= \bsy{\mu}_1 + \bsy{\Sigma}_{12} \bsy{\Sigma}_{22}^{-1} (\mathbf{x}_2 - \bsy{\mu}_2) \\
                \bsy{\Sigma}_{1|2 } &= \bsy{\Sigma}_{11} - \bsy{\Sigma}_{12} \bsy{\Sigma}_{22}^{-1} \bsy{\Sigma}_{21}.
            \end{align*}

        \item
            A affine transformation of $\mathbf{x}$ is normal.
            If $\mathbf{A} \in \mathbb{R}^{J \times K}$ and $\mathbf{b} \in \mathbb{R}^{J}$, then
            \begin{equation*}
                \mathbf{Ax} + \mathbf{b} \sim \mathcal{N}_J(
                \mathbf{A} \bsy{\mu} + \mathbf{b},
                \mathbf{A} \bsy{\Sigma} \mathbf{A}^{T}
                ).
            \end{equation*}

        \item
            If $\mathbf{y} \sim \mathcal{N}_J( \bsy{\mu}_y, \bsy{\Sigma}_{y})$ and $\mathbf{x}$ are independent, then
            \begin{equation*}
                \begin{bmatrix}
                    \mathbf{x} \\ \mathbf{y}
                \end{bmatrix} \sim \mathcal{N}_{K + J} \left(
                \begin{bmatrix}
                    \bsy{\mu} \\ \bsy{\mu}_{y}
                \end{bmatrix},
                \begin{bmatrix}
                    \bsy{\Sigma} & \mathbf{0}_{K \times J} \\
                    \mathbf{0}_{J \mathbf{x} K} & \bsy{\Sigma}_y
                \end{bmatrix}\right).
            \end{equation*}
    \end{enumerate}
\end{theorem}
\begin{proof}
    See Section 4 of \cite{murphy2012} for (a)-(c). For part (d) \textcolor{red}{TODO}
\end{proof}
