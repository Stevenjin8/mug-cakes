\subsection{Kernels}

Then central idea of Gaussian Processes is that if $\mathbf{x}$ and $\mathbf{x}'$ are similar,
then $f(\mathbf{x})$ and $f(\mathbf{x}')$ should be similar.
Kernels give us a way to quantify the similarity between two points.
(Note that this is different from the kernel of a functions, the kernel of a distribution, and the kernel of an operating system.)
%Given that most often we set the $m$ parameter of a Gaussian Process to be 0, there is only one parameter left.
In this section we explore the Radial Basis Function (RBF) kernel and derive some theoretical results.

Strictly speaking, a kernel is just a function $\kappa: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$,
but it is also usually nonnegative and symmetric.
For the rest of the paper, we will exclusive work with RBF kernels.

\begin{definition}[Radial Basis Function]
    A radial basis function $\kappa: \mathbb{R}^{k} \times \mathbb{R}^{k} \to \mathbb{R}$
    is given by
    \begin{equation*}
        \kappa(\mathbf{x}, \mathbf{x}'; \sigma_f^2, \mathbf{V})
        = \sigma_f^{2} \exp \left\{ -\frac12 (\mathbf{x} - \mathbf{x}')^{T} \mathbf{V}^{-1} (\mathbf{x} - \mathbf{x}') \right\}
    \end{equation*}
    for $\sigma_f^2 > 0$ and positive definite $V$.
    We reduce the number of parameters by settings $\mathbf{V} = \ell^2\mathbf{I}$ for $\ell^2 > 0$.
    Then,
    \begin{equation*}
        \kappa(\mathbf{x}, \mathbf{x}'; \sigma_f^2, \ell^2)
        = \sigma_f^{2} \exp \left\{ - \frac{ \lVert \mathbf{x} - \mathbf{x}' \rVert^2 }{ 2 \ell^2 }\right\}.
    \end{equation*}
    If the parameters are clear from context or not relevant, we omit them and write $\kappa(\mathbf{x}, \mathbf{x}')$ instead.
\end{definition}

An important class of kernel are Mercer kernels.
Such kernels allow us to implicitly map our data to a higher (potentially infinite) dimension inner product space, and work in that potentially infinite dimensional space without needing to infinite amount of computation.
We will use this definition to show prove necessary and desirable properties of RBF kernels which will make them good kernels for Gaussian Processes.

\begin{definition}[Mercer Kernel]
    A kernel $\kappa: \mathbb{R}^{k} \times \mathbb{R}^{k} \to \mathbb{R}$
    is said to be a Mercer Kernel if there exists an inner product space
    $V$ and mapping $\varphi: \mathbb{R}^{k} \to V$ such that
    \begin{equation*}
        \kappa(\mathbf{x}, \mathbf{x}') = \langle \varphi(\mathbf{x}), \varphi(\mathbf{x}') \rangle.
    \end{equation*}
\end{definition}

It might be surprising that RBF kernels are Mercer kernels if we are used to thinking of inner products as synonymous with dot products.
After all, there is no sum the formula of the RBF kernel.
The next few results utilize infinite dimensional vector spaces and a clever use of the Taylor expansion of $e^{x}$ to show that the RBF kernels is a Mercer kernel.

\begin{definition}[$\ell^1$]
    The set $\ell^{1}$ is the set of sequences of $\mathbb{R}$ whose infinite sum converges absolutely.
    That is
    \begin{equation*}
        \ell^{1} = \left\{ (x_n)_{n=0}^{\infty} \in \mathbb{R}^{\omega} \middle| \sum_{n=0}^{\infty} \lvert x_n \rvert < \infty \right\}.
    \end{equation*}
    For any two $\mathbf{f} = (f_n), \mathbf{g} = (g_n) \in \ell^{1}$, their inner product is
    \begin{equation*}
        \langle \mathbf{f}, \mathbf{g} \rangle = \sum\limits_{n=0}^{\infty} f_ng_n.
    \end{equation*}
\end{definition}
\begin{proposition}
    $\ell^{1}$ is an inner product space.
\end{proposition}
\begin{theorem}
    The RBF kernel is a Mercer kernel.
\end{theorem}
%\begin{proof}
%    Outline of the proof: We will first show that
%    \begin{equation*}
%        \ell^{1} = \{ \text{sequences of real numbers whose series converges absolutely} \}
%    \end{equation*}
%    is a vector space.
%    Then, we will induce a inner product on $\ell_1$.
%    Finally, we will show that there exists a $\varphi: \mathbb{R}^{K} \to \ell^{2}$ such that $\kappa(\mathbf{x}, \mathbf{y}) = \langle \varphi(\mathbf{x}), \varphi(\mathbf{y}) \rangle$.
%
%    For $\mathbf{x} = (x_n), \mathbf{y} = (y_n) \in \ell^{1}$ and $\alpha \in \mathbb{R}$, we define addition and scalar multiplication as usual.
%    \begin{align*}
%        \mathbf{x} + \mathbf{y} & = (x_n + y_n) \\
%        \alpha \mathbf{x} & = (\alpha x_n) \\
%    \end{align*}
%    This is clearly a vector space.
%
%    We define the inner product of $\mathbf{y}, \mathbf{y}$ as
%    \begin{equation*}
%        \langle \mathbf{x}, \mathbf{y} \rangle
%        = \sum\limits_{n=1}^{\infty} x_n y_n
%    \end{equation*}
%    We must first show that this function is well defined for any $\mathbf{x}, \mathbf{y} \in \ell^{1}$.
%    Because $\sum y_n$ converges, we know that $(y_n) \to 0$ and there exists an $M \in \mathbb{R}$ such that $\lvert y_n \rvert < M$ for all $n$.
%    It follows that
%    \begin{equation*}
%        \sum\limits_{n=1}^{\infty} \lvert x_n y_n \rvert
%        = \sum\limits_{n=1}^{\infty} \lvert x_n \rvert \lvert y_n \rvert
%        < \sum\limits_{n=1}^{\infty} \lvert x_n \rvert  M
%        = M\sum\limits_{n=1}^{\infty} \lvert x_n \rvert
%        < \infty
%    \end{equation*}
%    The last inequality comes from the fact that $\mathbf{x} \in \ell^{1}$.
%
%    Now, we show that we are working with a valid inner product using Definition 8.1 of \cite{axler2020}.
%    Suppose that $\mathbf{f}, \mathbf{g}, \mathbf{h} \in \ell^{1}$.
%    \begin{itemize}
%        \item
%            \textbf{Positivity}:
%            \begin{align*}
%                \langle \mathbf{f}, \mathbf{f} \rangle
%                = \sum f_{n}^2
%                \geq 0
%            \end{align*}
%            because all the terms in the sum are nonnegative.
%
%        \item
%            \textbf{Definiteness}:
%            \begin{align*}
%                \langle \mathbf{f}, \mathbf{f} \rangle = \sum f_n^2= 0
%                \iff f_n = 0 \quad \forall n
%                \iff \mathbf{f} = \mathbf{0}
%            \end{align*}
%
%        \item
%            \textbf{Linearity in first slot}:
%            \begin{align*}
%                \langle \mathbf{f} + \mathbf{g}, \mathbf{h} \rangle
%                = \sum (f_n + g_n)h_n
%                = \sum (f_nh_n + g_nh_n)
%                = \sum f_nh_n + \sum g_nh_n
%                = \langle \mathbf{f}, \mathbf{h} \rangle + \langle \mathbf{g}, \mathbf{h} \rangle
%            \end{align*}
%            We can split up the sum in the second last inequality because all $\sum f_nh_n, \sum g_nh_n$ converge absolutely.
%
%        \item
%            \textbf{Symmetry}:
%            \begin{align*}
%                \langle \mathbf{f}, \mathbf{g} \rangle
%                = \sum f_n + g_n
%                = \sum g_n + f_n
%                = \langle \mathbf{g}, \mathbf{f} \rangle
%            \end{align*}
%            We can split up the sum in the second last inequality because all $\sum f_n, \sum g_n$ converge absolutely.
%    \end{itemize}
%
%    Now, we show that RBF is a Mercer Kernel.
%    We first consider the case where $\mathbf{x}, \mathbf{y} \in \mathbb{R}^{1}$.
%    Since $\mathbf{x}, \mathbf{y}$ are scalars, we write them without boldface from now on.
%    Let $\varphi: \mathbb{R}^{1} \to \ell^{1}$ be given by
%    \begin{equation*}
%        \varphi(x) = \left(\sigma_f \exp\{-\frac12 x^2 / \ell^2\} \frac{ x^{n} }{ \ell\sqrt{n!} }\right)_{n=0}^{\infty}
%    \end{equation*}
%    and we adopt the convention that $0^{0} = 1$.
%    We know that the sum of the entries of $\varphi(x)$ converges absolutely by doing the ratio test
%    \begin{equation*}
%        \lim_{n \to \infty} \frac{ \sigma_f \exp\{-\frac12x^2 / \ell^2\}(x/\ell)^{n+1} / \sqrt{(n + 1)!}}
%        { \sigma_f \exp\{-\frac12x^2 / \ell^2\}(x/\ell)^{n} / \sqrt{n!} }
%        = \lim \frac{ x/\ell^2 }{ \sqrt{n + 1} }
%        = 0
%    \end{equation*}
%    It follows that
%    \begin{align*}
%        \kappa(x, y) = \sigma_f^{2} \exp\left\{-\frac12 \frac{ (x - y)^2 }{ \ell^2 }\right\}
%        &= \sigma_f \exp\{-\frac12 \frac{ x^2 }{ \ell^2 }\}
%        \sigma_f \exp\{-\frac12 \frac{ y^2 }{ \ell^2 }\}
%        \exp \{ \frac{ xy }{ \ell^2 } \} \\
%        &= \sigma_f \exp\{-\frac12 \frac{ x^2 }{ \ell^2 }\}
%        \sigma_f \exp\{-\frac12 \frac{ y^2 }{ \ell^2 }\}
%        \sum_{n=0}^\infty \frac{ xy / \ell^2 }{ n! } \\
%        &= \sigma_f \exp\{-\frac12 \frac{ x^2 }{ \ell^2 }\}
%        \sigma_f \exp\{-\frac12 \frac{ y^2 }{ \ell^2 }\}
%        \sum_{n=0}^\infty \frac{ x / \ell}{ \sqrt{n!} }\frac{ y / \ell}{ \sqrt{n!} } \\
%        &=  \langle \varphi(x), \varphi(y) \rangle
%    \end{align*}
%    
%    
%    
%
%
%
%
%\end{proof}

Using the fact that the RBF kernel is a Mercer Kernel, we can show that it generates positive semi definite Gram matrices.
\begin{lemma}
    Mercer Kernels are positive semi-definite.
\end{lemma}
\begin{proof}

\end{proof}

\begin{corollary}
    The RBF kernel is positive semi-definite.
\end{corollary}

In fact, if our $\mathbf{x}_1, \ldots, \mathbf{x}_n$ are distinct, the RBF kernel will generate a positive definite matrix.
This makes sense because for our resulting Gaussian Process will have support for any continuous function.

\begin{lemma}
    If $\mathbf{x}_1, \ldots, \mathbf{x}_n$ are distinct, then the Gram matrix generated by an RBF kernel will be positive definite.
\end{lemma}
\begin{proof}

\end{proof}
