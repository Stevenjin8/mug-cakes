\subsection{Kernels}\label{ssec:kernel}

The central idea behind Bayesian optimization is that learning about $f(\mathbf{x})$ gives us information about its nearby points allowing us to make inferences about $f$'s global behavior with a finite number of observations.
Modeling $f$'s global behavior with a surrogate model $F \sim \mathcal{GP}(m, \kappa)$ will allow us to make inferences on the location of peaks which will aid our acquisition function in balancing exploration and exploitation.
Gaussian processes implement the idea of similarity with a kernel which gives the covariance of $F(\mathbf{x})$ and $F(\mathbf{x}')$ as a function of $\mathbf{x} \in \mathcal{X}$ and $\mathbf{x}' \in \mathcal{X}$.
In this section, we will study the Radial Basis Function and show that it satisfies theoretical requirements of Gaussian processes and work well with Bayesian optimization.

\begin{definition}[Radial Basis Function]\label{def:rbf}
    A radial basis function $\kappa_{\rbf}: \mathbb{R}^{N} \times \mathbb{R}^{N} \to \mathbb{R}$
    is given by
    \begin{equation}\label{eq:rbf}
        \kappa_{\rbf}(\mathbf{x}, \mathbf{x}'; \sigma_f^2, \ell^2)
        = \sigma_f^{2} \exp \left\{ - \frac{ \lVert \mathbf{x} - \mathbf{x}' \rVert^2 }{ 2 \ell^2 }\right\}
    \end{equation}
    for parameters $\sigma_f^2 > 0$ and $\ell^2 > 0$.

    If the parameters are clear from context or not relevant, we omit them and write $\kappa_{\rbf}(\mathbf{x}, \mathbf{x}')$ instead.
\end{definition}
In the context of Gaussian processes, $\sigma_f^2$ gives us the prior marginal variance of $F(\mathbf{x})$ for all $\mathbf{x}$ because
\begin{equation*}
    \Var(F(\mathbf{x})) = \Cov(F(\mathbf{x}), F(\mathbf{x})) = \kappa_{\rbf}(\mathbf{x}, \mathbf{x}) = \sigma_f^2.
\end{equation*}
The parameter $\ell^2$ scales the domain by dividing $\lVert \mathbf{x} - \mathbf{x}' \rVert$.
Large values of $\ell^2$ shrink the domain while small values of $\ell^2$ expand the domain.
For example, if our $\mathcal{X}$-units were in cups, we would choose small $\ell^2$, but if the units were in teaspoons, we would choose large $\ell^2$.

We also see that RBF uses Euclidean distance as a measure of similarity.
If $\lVert \mathbf{x} - \mathbf{x}' \rVert^2$ is large, then the correlation between $F(\mathbf{x})$ and $F(\mathbf{x}')$ small:
\begin{equation*}
    \frac{\Cov(F(\mathbf{x}), F(\mathbf{x}'))}{\sqrt{\Var[F(\mathbf{x})]\Var[F(\mathbf{x}')])}} = \frac{\kappa_{\rbf}(\mathbf{x}, \mathbf{x}')}{\sigma_f^2} = \exp \left\{ -\frac{ \lVert \mathbf{x} - \mathbf{x}' \rVert^2 }{ 2\ell^2 } \right\} \approx 0.
\end{equation*}
However, if $\lVert \mathbf{x} - \mathbf{x}' \rVert^2$ is small, then the correlation is large:
\begin{equation*}
    \frac{\Cov(F(\mathbf{x}), F(\mathbf{x}'))}{\sqrt{\Var[F(\mathbf{x})]\Var[F(\mathbf{x}')])}}
    = \frac{\kappa_{\rbf}(\mathbf{x}, \mathbf{x}')}{\sigma_f^2} = \exp \left\{ -\frac{ \lVert \mathbf{x} - \mathbf{x}' \rVert^2 }{ 2\ell^2 } \right\} \approx 1.
\end{equation*}

An important class of kernels is Mercer kernels.
Such kernels allow us to implicitly map our data to a higher (potentially infinite) dimensional inner product space, and work in that potentially infinite dimensional space with a finite amount of computation.
We will use this definition to prove necessary and desirable properties of RBF kernels which will make them suitable kernels for Gaussian processes.

\begin{definition}[Mercer Kernel]
    A kernel $\kappa: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$
    a Mercer Kernel if there exists an inner product space
    $V$ and mapping $\gamma : \mathcal{X} \to V$ such that
    \begin{equation*}
        \kappa(\mathbf{x}, \mathbf{x}') = \langle \gamma(\mathbf{x}), \gamma(\mathbf{x}') \rangle.
    \end{equation*}
\end{definition}

It might be surprising that RBF kernels are Mercer kernels because there is no sum in Equation~\ref{eq:rbf}.
The next few results utilize infinite dimensional vector spaces and the Taylor expansion of $\exp$ to show that RBF kernels are Mercer kernels.
For the next definition, $\mathbb{R}^{\omega}$ is the set of sequences of real numbers.

\begin{definition}[$L^1$]
    The set $L^1$ is the set of sequences of $\mathbb{R}$ whose infinite sum converges absolutely.
    That is
    \begin{equation*}
        L^1 = \left\{ (f_n)_{n=1}^{\infty} \in \mathbb{R}^{\omega} \middle| \sum_{n=1}^{\infty} \lvert f_n \rvert < \infty \right\}.
    \end{equation*}
    For any two $\mathbf{f} = (f_n), \mathbf{g} = (g_n) \in L^1$, their inner product is
    \begin{equation*}
        \langle \mathbf{f}, \mathbf{g} \rangle = \sum\limits_{n=1}^{\infty} f_ng_n.
    \end{equation*}
\end{definition}
\begin{proposition}
    $L^1$ is an inner product space.
\end{proposition}
\begin{proof}
    We need to show that $L^1$ is a vector space and $\langle \cdot, \cdot \rangle$ is an inner product.

    The set of sequence of real numbers $\mathbb{R}^{\omega}$ is a vector space by Example 6.29 of \cite{axler2020}.
    To show that $L^1 \subseteq \mathbb{R}^{\omega}$ is a vector space, we just need to show that it is a subspace of $\mathbb{R}^{\omega}$.
    \begin{itemize}
        \item \textbf{additive identity}\\
            $\mathbf{0} = (0, 0, \dots)$ is an element of $L^1$ because
            $\sum\limits_{n=1}^{\infty} \lvert 0 \rvert = 0 < \infty$.
        \item \textbf{closed under addition} \\
            Let $\mathbf{f}, \mathbf{g} \in L^1$.
            Then,
            \begin{align*}
                \sum\limits_{n=1}^{\infty} \lvert f_n + g_n \rvert
                \leq \sum\limits_{n=1}^{\infty} \lvert f_n \rvert + \lvert g_n \rvert
                = \sum\limits_{n=1}^{\infty} \lvert f_n \rvert + \sum\limits_{n=1}^{\infty} \lvert g_n \rvert
                < \infty
            \end{align*}
            meaning that $\mathbf{f} + \mathbf{g} \in L^1$.
        \item \textbf{closed under scalar multiplication} \\
            Let $\mathbf{f} \in L^1$ and $\alpha \in \mathbb{R}$.
            Then
            \begin{equation*}
                \sum\limits_{n=1}^{\infty} \lvert \alpha f_n \rvert
                = \lvert \alpha \rvert \sum\limits_{n=1}^{\infty} \lvert f_n \rvert
                < \infty
            \end{equation*}
            meaning that $\alpha \mathbf{f} \in L^1$.
    \end{itemize}
    Thus, $L_1$ is a vector space.

    Now, we show that $\langle \cdot , \cdot \rangle$ is an inner product by Definition 8.1 of \cite{axler2020}.
    \begin{itemize}
        \item \textbf{positivity}\\
            Let $\mathbf{f} \in L^1$.
            Then
            \begin{equation*}
                \langle \mathbf{f}, \mathbf{f} \rangle
                = \sum\limits_{n=1}^{\infty} f_n^2
                \geq 0.
            \end{equation*}
            At the same time, $\mathbf{f}$ must be bounded because the infinite sum of its elements converges, so there exists $M \in \mathbb{R}$ such that $M > \lvert f_n \rvert$ for all $n \in \mathbb{Z}^{+}$.
            It follows that
            \begin{equation*}
                \langle \mathbf{f}, \mathbf{f} \rangle
                = \sum\limits_{n=1}^{\infty} f_n^2
                \leq \sum\limits_{n=1}^{\infty}
                M \lvert f_n \rvert < \infty
            \end{equation*}

        \item \textbf{definiteness} \\ Let $\mathbf{f} \in L^1$.
            Then,
            \begin{align*}
                \langle \mathbf{f}, \mathbf{f} \rangle = 0
                \iff
                \sum\limits_{n=1}^{\infty} f_n^2 = 0
                \iff
                \mathbf{f} = \mathbf{0}.
            \end{align*}

        \item \textbf{linearity in first slot} \\
            Let $\mathbf{f}, \mathbf{g}, \mathbf{h} \in L^1$.
            Then,
            \begin{align*}
                \langle \mathbf{f} + \mathbf{g}, \mathbf{h} \rangle
                = \sum\limits_{n=1}^{\infty} (f_n + g_n)h_n
                = \sum\limits_{n=1}^{\infty} f_ng_n + g_nh_n
                = \sum\limits_{n=1}^{\infty} f_nh_n +\sum\limits_{n=1}^{\infty} g_nh_n
                = \langle \mathbf{f}, \mathbf{h} \rangle + \langle \mathbf{g}, \mathbf{h} \rangle.
            \end{align*}

        \item \textbf{symmetry} \\
            Let $\mathbf{f}, \mathbf{g}, \in L^1$.
            Then,
            \begin{equation*}
                \langle \mathbf{f}, \mathbf{g} \rangle
                = \sum\limits_{n=1}^{\infty} f_ng_n
                = \sum\limits_{n=1}^{\infty} g_nf_n
                = \langle \mathbf{g}, \mathbf{f} \rangle.
            \end{equation*}
    \end{itemize}
\end{proof}

\begin{theorem}
    \label{thm:rbf-mercer}
    RBF kernels are Mercer kernels.
\end{theorem}
\begin{proof}
    This proof is inspired by \cite{shashua2009}.
    For notational simplicity, we only consider the case where $\mathbf{x} \in \mathcal{X} \subseteq \mathbb{R}^{1}$.
    Let $\gamma : \mathbb{R}^{1} \to L^1$ be
    \begin{equation*}
        \gamma(\mathbf{x}) = \sigma_f \exp\left\{- \frac{\mathbf{x}^2}{2\ell^2}\right\} \left(\frac{ (\mathbf{x} / \ell)^{n - 1} }{ \sqrt{(n - 1)!} }\right)_{n=1}^{\infty}.
    \end{equation*}
    (We adopt the convention that $0^0 = 1$).
    The image of $\gamma$ is $L_1$ because the absolute sum of the elements of $\gamma(\mathbf{x})$ converges by the ratio test.
    Then, for $\mathbf{x}, \mathbf{x'} \in \mathbb{R}^{1}$,
    \begin{align*}
        \langle \gamma(\mathbf{x}), \gamma(\mathbf{x'}) \rangle
        & = \sigma_f^2
        \exp \left\{ -\frac{\mathbf{x}^2}{2\ell^2} \right\}
        \exp \left\{ -\frac{ \mathbf{x'}^2}{2\ell^2} \right\}
        \sum\limits_{n=1}^{\infty}
        \frac{ (\mathbf{x}/\ell)^{n - 1} (\mathbf{x'}/\ell)^{n - 1}}{ (n - 1)! } \\
        & = \sigma_f^2
        \exp \left\{ -\frac{\mathbf{x}^2}{2\ell^2} \right\}
        \exp \left\{ -\frac{ \mathbf{x'}^2}{2\ell^2} \right\}
        \sum\limits_{n=0}^{\infty}
        \frac{ \left(\mathbf{x}\mathbf{x'}/\ell^2\right)^{n }}{ n ! } \\
        & = \sigma_f^2
        \exp \left\{ -\frac{ \mathbf{x}^2}{2\ell^2} \right\}
        \exp \left\{ -\frac{ \mathbf{x'}^2}{2\ell^2} \right\}
        \exp \left\{ \frac{\mathbf{x} \mathbf{x'}}{ \ell^2} \right\} \\
        & = \sigma_f^2 \exp \left\{ -\frac{ \lVert \mathbf{x} - \mathbf{x'} \rVert^2 }{ 2\ell^2 } \right\} \\
        & = \kappa_{\rbf}(\mathbf{x}, \mathbf{x'}).
    \end{align*}

    The proof becomes more laborious when considering $\mathcal{X} \subseteq \mathbb{R}^{K}$ because of the multinomial expansion of $\lVert \mathbf{x} - \mathbf{x}' \rVert^2$.
    For a complete proof, see Section 4.3.3 of \cite{shashua2009}.
\end{proof}

Using the fact that RBF kernels are Mercer Kernels, we can show that they generate positive semi-definite covariance matrices.

\begin{lemma}
    Suppose $\kappa: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is a Mercer kernel.
    Then, for any $N \in \mathbb{Z}^{+}$ and $\mathbf{x}_1, \dots, \mathbf{x}_N \in \mathcal{X}$, the matrix $\mathbf{K}$
    given by
    \begin{equation*}
        (\mathbf{K})_{ij} = \kappa(\mathbf{x}_i, \mathbf{x}_j)
    \end{equation*}
    is positive semi-definite.
\end{lemma}
\begin{proof}
    Suppose that $\kappa: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is a Mercer Kernel.
    Then, there exists a inner product space $V$ and function $\gamma: \mathcal{X} \to V$ such that $\kappa(\mathbf{x}, \mathbf{x}') = \langle\gamma(\mathbf{x}), \gamma(\mathbf{x}')\rangle$.

    Now, let $\mathbf{x}_1, \dots, \mathbf{x}_N \in \mathcal{X}$, $\mathbf{c} \in \mathbb{R}^{N}$, and
    \begin{equation*}
        \mathbf{K} =
        \begin{bmatrix}
            \kappa(\mathbf{x}_1, \mathbf{x}_1) & \dots & \kappa(\mathbf{x}_1, \mathbf{x}_N) \\
            \vdots & \ddots & \vdots \\
            \kappa(\mathbf{x}_N, \mathbf{x}_1) & \dots & \kappa(\mathbf{x}_N, \mathbf{x}_N) \\
        \end{bmatrix}
        .
    \end{equation*}
    Then,
    \begin{align*}
        \mathbf{c}^{T} \mathbf{K} \mathbf{c}
        & = \left\langle \sum_{i = 1}^{N}c_i \gamma(\mathbf{x}_i),
        \sum_{i = 1}^{N}c_i \gamma(\mathbf{x}_i) \right\rangle
        \geq 0
    \end{align*}
    by the positivity of inner products.
\end{proof}

\begin{corollary}
    \label{corr:rbf-psd}
    RBF kernels are positive semi-definite.
\end{corollary}

In fact, if $\mathbf{x}_1, \dots, \mathbf{x}_N$ are distinct, then $\mathbf{K}$ is strictly positive definite as the following result shows.

\begin{lemma}
    \label{lem:rbf-pd}
    If $\mathbf{x}_1, \dots, \mathbf{x}_N$ are distinct, then the covariance matrix generated $\mathbf{K}, (\mathbf{K})_{ij} = \kappa(\mathbf{x}_i, \mathbf{x}_j)$ by an RBF kernel is strictly positive definite.
\end{lemma}
\begin{proof}
    Once again, we only consider the case where $\mathbf{x}, \mathbf{y} \in \mathcal{X} \subseteq \mathbb{R}^{1}$.
    Without loss of generality, assume that $\sigma_f^2 = \ell^2 = 1$.
    We can write $\kappa_{\rbf}(\mathbf{x}, \mathbf{y}) = \langle \gamma(\mathbf{x}), \gamma(\mathbf{y}) \rangle$
    where $\gamma: \mathcal{X} \to L^{1}$ is given by
    \begin{equation*}
        \gamma(\mathbf{x}) = \exp\left\{-\frac{\mathbf{x}^2}2 \right\} \left(\frac{ \mathbf{x}^{n - 1} }{ \sqrt{(n - 1)!} }\right)_{n=1}^{\infty}
    \end{equation*}
    as in the proof for Theorem~\ref{thm:rbf-mercer}.

    First, we show that if $\mathbf{x}_1, \dots, \mathbf{x}_N$ are distinct, $\gamma(\mathbf{x}_1), \dots, \gamma(\mathbf{x}_N)$ are linearly independent.
    For contradiction, assume that $\gamma(\mathbf{x}_1), \dots, \gamma(\mathbf{x}_N)$ are linearly dependent.
    Then, there exists an $M \in \mathbb{Z}^{+}, 0 < M < N$ such that $\gamma(\mathbf{x}_1), \dots, \gamma(\mathbf{x}_M)$
    are linearly independent and for some weights $w_1, \dots, w_M \in \mathbb{R}$,
    \begin{equation}
        \label{eq:dep}
        \sum_{i = 1}^{M}w_i \gamma(\mathbf{x}_i)  = \gamma(\mathbf{x}_{M + 1}).
    \end{equation}
    Since $\gamma$ does not map to $\mathbf{0}$, we know that $M > 0$ and there exists a $w_p \neq 0$ with $p \in \mathbb{Z}^{+}$, $0 < p \leq M$.

    Expanding out at Equation~\ref{eq:dep},
    \begin{align*}
        \sum_{i = 1}^{M}w_i \exp\left\{-\frac{\mathbf{x}_{i}^2}2 \right\} \left(\frac{ \mathbf{x}_i^{n - 1} }{ \sqrt{(n - 1)!} }\right)_{n=1}^{\infty}
        & =
        \exp\left\{-\frac{\mathbf{x}_{M + 1}^2}2 \right\} \left(\frac{ \mathbf{x}_{M + 1}^{n - 1} }{ \sqrt{(n - 1)!} }\right)_{n=1}^{\infty} \\
        \implies
        \sum_{i = 1}^{M}w_i \exp\left\{-\frac{\mathbf{x}_{i}^2}2 \right\} \left(\frac{ \mathbf{x}_i^{n - 1} }{ \sqrt{(n - 1)!} }\right)_{n=2}^{\infty}
        & =
        \exp\left\{-\frac{\mathbf{x}_{M + 1}^2}2 \right\} \left(\frac{ \mathbf{x}_{M + 1}^{n - 1} }{ \sqrt{(n - 1)!} }\right)_{n=2}^{\infty} \\
        \implies
        \sum_{i = 1}^{M}w_i \exp\left\{-\frac{\mathbf{x}_{i}^2}2 \right\} \left( \frac{ \mathbf{x}_i^{n - 1} }{ \sqrt{(n - 2)!} }\right)_{n=2}^{\infty}
        & =
        \exp\left\{-\frac{\mathbf{x}_{M + 1}^2}2 \right\} \left(\frac{ \mathbf{x}_{M + 1}^{n - 1} }{ \sqrt{(n - 2)!} }\right)_{n=2}^{\infty} \\
        \implies
        \sum_{i = 1}^{M}w_i \exp\left\{-\frac{\mathbf{x}_{i}^2}2 \right\} \mathbf{x}_i \left( \frac{ \mathbf{x}_i^{n - 2} }{ \sqrt{(n - 2)!} }\right)_{n=2}^{\infty}
        & =
        \exp\left\{-\frac{\mathbf{x}_{M + 1}^2}2 \right\} \mathbf{x}_{M + 1}\left(\frac{ \mathbf{x}_{M + 1}^{n - 2} }{ \sqrt{(n - 2)!} }\right)_{n=2}^{\infty} \\
        \implies
        \sum_{i = 1}^{M}\mathbf{x}_i w_i \exp\left\{-\frac{\mathbf{x}_{i}^2}2 \right\} \left( \frac{ \mathbf{x}_i^{n - 1} }{ \sqrt{(n - 1)!} }\right)_{n=1}^{\infty}
        & =
        \mathbf{x}_{M + 1}\exp\left\{-\frac{\mathbf{x}_{M + 1}^2}2 \right\}  \left(\frac{ \mathbf{x}_{M + 1}^{n - 1} }{ \sqrt{(n - 1)!} }\right)_{n=1}^{\infty} \\
        \implies
        \sum\limits_{n=1}^{M} \mathbf{x}_n w_n \gamma(\mathbf{x}_n) & = \mathbf{x}_{M + 1} \gamma(\mathbf{x}_{M + 1}).
    \end{align*}

    If $\mathbf{x}_{M + 1} = 0$
    \begin{equation*}
        \sum\limits_{n=1}^{M} \mathbf{x}_n w_n \gamma(\mathbf{x}_n) = \mathbf{0}.
    \end{equation*}
    We know that $\mathbf{x}_p w_p \neq 0$ because $w_p \neq 0$ and $\mathbf{x}_p \neq \mathbf{x}_{M + 1} = 0$.
    As such, we have found a linear combination of $\gamma(\mathbf{x}_1), \dots, \gamma(\mathbf{x}_M)$ with some nonzero weight that equals $\mathbf{0}$ contradicting our assumption that $\gamma(\mathbf{x}_1), \dots, \gamma(\mathbf{x}_M)$ are linearly independent.

    If $\mathbf{x}_{M + 1} \neq 0$,
    \begin{align}
        & \sum\limits_{n=1}^{M} \mathbf{x}_n w_n \gamma(\mathbf{x}_n) = {\mathbf{x}_{M + 1}}\gamma(\mathbf{x}_{M + 1}) \\
        \implies & \sum\limits_{n=1}^{M} \frac{\mathbf{x}_n}{\mathbf{x}_{M + 1}} w_n \gamma(\mathbf{x}_n) = \gamma(\mathbf{x}_{M + 1}). \label{eq:other-dep}
    \end{align}
    Subtracting Equations \ref{eq:dep} and \ref{eq:other-dep},
    \begin{equation*}
        \sum\limits_{n=1}^{M} w_n \left(\frac{\mathbf{x}_n}{\mathbf{x}_{M + 1}} - 1\right) \gamma(\mathbf{x}_n) = \gamma(\mathbf{x}_{M + 1}) - \gamma(\mathbf{x}_{M + 1}) = \mathbf{0}.
    \end{equation*}
    Since $\mathbf{x}_p \neq \mathbf{x}_{M + 1}$, we conclude that
    \begin{equation*}
        w_p\left(\frac{ \mathbf{x}_p }{ \mathbf{x}_{M + 1} } - 1\right) \neq 0.
    \end{equation*}
    Once again, we have found a linear combination of $\gamma(\mathbf{x}_1), \dots, \gamma(\mathbf{x}_M)$ with some nonzero weights that equals $\mathbf{0}$ contradicting the assumption that $\gamma(\mathbf{x}_1), \dots, \gamma(\mathbf{x}_M)$ are linearly independent.
    Since we get a contradiction in both cases, we conclude that $\gamma(\mathbf{x}_1), \dots, \gamma(\mathbf{x}_N)$ are linearly independent when $\mathbf{x}_1, \dots, \mathbf{x}_N$ are distinct.

    Now, we use the independence of $\gamma(\mathbf{x}_1), \dots, \gamma(\mathbf{x}_N)$ to show that $\mathbf{K}$ is strictly positive definite.
    Let $\mathbf{c} \in \mathbb{R}^{N}$ be nonzero.
    Since $\gamma(\mathbf{x}_1), \dots, \gamma(\mathbf{x}_N)$ are linearly independent,
    \begin{equation*}
        \sum\limits_{n = 1}^{N} c_n \gamma(\mathbf{x}_n) \neq \mathbf{0}.
    \end{equation*}
    It follows that
    \begin{align*}
        \mathbf{c}^{T} \mathbf{K} \mathbf{c}
        = \left\langle \sum\limits_{n = 1}^{N} c_n \gamma(\mathbf{x}_n), \sum\limits_{n = 1}^{N} c_n \gamma(\mathbf{x}_n) \right\rangle
        > 0
    \end{align*}
    by the definiteness and positivity properties of inner products.
    Thus, RBF kernels generate strictly positive definite covariance matrix $\mathbf{K}$ when $\mathbf{x}_1, \dots, \mathbf{x}_N$ are distinct.
\end{proof}

This theorem shows that, the distribution of $(F(\mathbf{x}_1), \dots F(\mathbf{x}_N))$ will be nondegenerate for $N \in \mathbb{Z}^{+}$.
Given any distinct $\mathbf{x}_1, \dots \mathbf{x}_N$, and $y_1, \ldots, y_N \in \mathbb{R}$, we allow the possibility that
\begin{equation*}
    f(\mathbf{x}_1) = y_1, \dots, f(\mathbf{x}_N) = y_N.
\end{equation*}
Therefore, by modeling our beliefs about $f$ with a Gaussian process $F \sim \mathcal{GP}(m, \kappa_{\rbf})$,
we can perform posterior inference on $F$ without making parametric assumptions about $f$'s shape.

