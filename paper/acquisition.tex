\subsection{Acquisition Functions}
As we see in Algorithm~\ref{alg:bo}, acquisition functions dictate which point we sample in each iteration.
As such, acquisition functions balance two competing interests in Bayesian Optimization: exploration and exploitation.
Exploration is searching far and wide in $\mathcal{X}$ as to understand $f$'s global behavior.
Exploitation is exploring local regions where we know $f$ is high to get more exact estimates of $f$'s peaks.
Too much exploration causes us to waste time by sampling regions of $\mathcal{X}$ with little potential.
Too much exploitation causes us to waste time by sampling regions of $\mathcal{X}$ that we already understand, thereby giving us little additional knowledge of $f$.

Unlike Gaussian Processes, acquisition functions are usually based on heuristics.
Common acquisition functions such as Expected Improvement and Probability of Improvement take a greedy approach, while information-based approaches that do look into the future are often intractable \cite{shahriari2016}.
For this paper, we will use a modified version of Expected Improvement.

%Suppose that after after the stopping conditions are met in our Bayesian Optimization loop (Algorithm \ref{algo:bo}),
%we return
%\begin{equation*}
%    \argmax_{\mathbf{x} \in \{ \mathbf{x}_1, \dots, \mathbf{x}_N \}} \mathbb{E}[f(\mathbf{x}) | \mathcal{D}_N].
%\end{equation*}
Under EI, the utility of a point $\mathbf{x}$ is what we expect the maximum $y$-value,
across all samples, to be after this iteration.
\begin{equation}
    \label{eq:ei}
    a_{EI}(\mathbf{x} | \mathcal{D}_n) = \mathbb{E}[\max\{f(\mathbf{x}), y_1, \dots, y_n\}]
    = y_* + \mathbb{E}[\max\{f(\mathbf{x}) - y_*, 0\}]
\end{equation}
where $y_* = \max{y_1, \dots, y_n}$.

However, the usage of $y_*$ in Equation~\ref{eq:ei} makes less sense if we have noisy observations because $y_*$'s extremity could due to noise.
It makes less sense if we consider biased observations because $y_*$ is even less representative of the underlying function and we are not using our knowledge about each observer's bias.
The following definition addresses this issues by replacing $y_*$ with $f(x_*)$.

\begin{definition}[vEI]
    \begin{equation}
        \label{eq:vei}
        a_{vEI}(\mathbf{x} | \mathcal{D}_N) = \mathbb{E}[\max \{ f(\mathbf{x}), f(\mathbf{x}_*) \} | \mathcal{D}_n]
        = \mathbb{E}[f(\mathbf{x}_*) | \mathcal{D}_N] + \mathbb{E}[\max \{ f(\mathbf{x}) - f(\mathbf{x}_*), 0 \} | \mathcal{D}_N]
    \end{equation}
    where $\mathbf{x}_* = \argmax_{\mathbf{x'} \in (\mathbf{x}_1, \dots, \mathbf{x}_N)} \mathbb{E}[\mathbf{x'} | \mathcal{D}_N]$.
\end{definition}

The big difference between vEI and EI is that while $y_*$ in Equation~\ref{eq:ei} is a constant, $f(\mathbf{x}_*)$ in Equation~\ref{eq:vei} is a random variable.
Thus, we need to take into account the dependence between $f(\mathbf{x})$ and $f(\mathbf{x}_*)$.
In practice this dependence encourages exploration because $f(\mathbf{x}) - f(\mathbf{x}_*)$ will be small when $\mathbf{x}$ and $\mathbf{x}_*$ are similar as seen in Figure~\ref{fig:ei}.
Even though our posterior maximum is at around 0.8, vEI is maximized at around 0.25 because 0.25 is further away from $\mathbf{x}_* = 0.75$ than 0.8.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{fig/ei.png}
    \caption{vEI given some data some unbiased but noisy observations.
        Dotted vertical red line represent sampled $\mathbf{x}$-value with the highest posterior mean.
    }
    \label{fig:ei}
\end{figure}

Because $f(\mathbf{x})$ and $f(\mathbf{x}_*)$ are jointly Gaussian for all $\mathbf{x}$, we can find vEI and its gradient in closed form.
Let $\varphi$ and $\Phi$ be the pdf and cdf of a standard univariate normal respectively. 
Since $f(\mathbf{x}), f(\mathbf{x}_*) | \mathcal{D}_n$ are jointly Gaussian
\begin{equation*}
    \begin{bmatrix}
        f(\mathbf{x}) \\
        f(\mathbf{x}_*)
    \end{bmatrix}
    | \mathcal{D}_n
    \sim \mathcal{N}\left(
    \begin{bmatrix}
            \mu_1 \\
            \mu_2
        \end{bmatrix}
    ,
    \begin{bmatrix}
            \sigma_{1}^2 & \sigma_{12} \\
            \sigma_{21} & \sigma^2_2
        \end{bmatrix}
    \right),
\end{equation*}
we can write
\begin{equation*}
    f(\mathbf{x}) - f(\mathbf{x}_*) \sim \mathcal{N}(\mu_1 - \mu_2, \sigma_1^2 + \sigma_2^2 - 2(\sigma_{12}))
    = \mathcal{N}(\mu, \sigma^2)
\end{equation*}
by Theorem~\ref{thm:ogag}.
Then,
\begin{align*}
    a_{vEI}(\mathbf{x} | \mathcal{D}_N)
    & =  \mathbb{E}[f(\mathbf{x}_*) + \max \{  f(\mathbf{x}) - f(\mathbf{x}_*), 0 \} | \mathcal{D}_N] \\
    & = \mu_2 + \int_{0}^\infty u \frac{ 1 }{ \sqrt{2 \pi } \sigma} \exp \left\{ -\frac12 \left(\frac{ u - \mu }{ \sigma }\right)^2 \right\} du \\
    & = \mu_2 + \int_{0}^\infty u \frac{ 1 }{ \sigma } \varphi\left(\frac{ u - \mu }{ \sigma }\right) du \\
    & = \mu_2 + \left[ \mu \Phi\left(\frac{ u - \mu }{ \sigma }\right) - \sigma \varphi\left(\frac{ u - \mu }{ \sigma }\right)\right]_{0}^{\infty} \\
    & = \mu_2 +  \mu - \mu \Phi\left(\frac{ -\mu }{ \sigma }\right) + \sigma \varphi\left(\frac{ -\mu }{ \sigma }\right) \\
    %& = \mu_2 + \mu - \mu\left( 1 - \Phi\left(\frac{ \mu }{ \sigma }\right)\right) + \sigma\varphi\left(\frac{ \mu }{ \sigma }\right) \\
    & = \mu_2 + \mu \Phi\left(\frac{ \mu }{ \sigma }\right) + \sigma \varphi\left(\frac{ \mu }{ \sigma }\right).
\end{align*}

Because of the usage of $\Phi$, it is hard to maximize $a_{vEI}$ it analytically.
At the same time, we can see in Figure~\ref{fig:ei}, $a_{vEI}$ has lots local maximum.
Thus, have to rely on methods such as Basin Hopping \cite{wales1997}.
Although this requires using a gradient, we omit the derivation because its just a messy application of the chain rule.
